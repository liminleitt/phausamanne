{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition with accelerometer data\n",
    "\n",
    "This demo shows how the `sklearn_xarray` package works with the `Pipeline` and `GridSearchCV` methods from scikit-learn providing a metadata-aware grid-searchable pipeline mechansism.\n",
    "\n",
    "The package combines the metadata-handling capabilities of `xarray` with the machine-learning framework of `sklearn`. It enables the user to apply preprocessing steps group by group, use transformers that change the number of samples, use metadata directly as labels for classification tasks and more.\n",
    "\n",
    "The example performs activity recognition from raw accelerometer data with a feedforward neural network. It uses the [WISDM activity prediction dataset](http://www.cis.fordham.edu/wisdm/dataset.php) which contains the activities walking, jogging, walking upstairs, walking downstairs, sitting and standing from 36 different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_xarray.dataarray as da\n",
    "from sklearn_xarray import Target\n",
    "from sklearn_xarray.preprocessing import (Splitter, Sanitizer, Featurizer)\n",
    "from sklearn_xarray.model_selection import CrossValidatorWrapper\n",
    "from sklearn_xarray.datasets import load_wisdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_wisdm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a pipeline with various preprocessing steps and a classifier.\n",
    "\n",
    "The preprocessing consists of splitting the data into segments, removing segments with `nan` values and standardizing.  Since the accelerometer data is three-dimensional but the standardizer and classifier expect a one-dimensional feature vector, we have to vectorize the samples.\n",
    "\n",
    "Finally, we use a feedforward neural network to perform the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('splitter', Splitter(\n",
    "        groupby=['subject', 'activity'], new_dim='timepoints')),\n",
    "    ('sanitizer', Sanitizer()),\n",
    "    ('featurizer', Featurizer()),\n",
    "    ('scaler', da.TransformerWrapper(StandardScaler())),\n",
    "    ('mlp', da.ClassifierWrapper(MLPClassifier(), reshapes='features'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use cross-validated grid search to find the best model\n",
    "parameters, we define a cross-validator. In order to make sure the model\n",
    "performs subject-independent recognition, we use a `GroupShuffleSplit`\n",
    "cross-validator that ensures that the same subject will not appear in both\n",
    "training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidatorWrapper(\n",
    "    GroupShuffleSplit(n_splits=3, test_size=0.3), groupby=['subject'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search will try different combinations of segment length and\n",
    "neural network layers to find the best parameters for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    pl, cv=cv, verbose=3, param_grid={\n",
    "        'splitter__new_len': [30, 60],\n",
    "        'mlp__estimator__hidden_layer_sizes': [(100,), (100, 50)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label to classify is the activity which we convert to a binary representation for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Target('activity', LabelBinarizer(), dims=['feature']).assign_to(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30, score=0.6250541829215431, total=  22.1s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30, score=0.6576610264560157, total=  19.5s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   46.0s remaining:    0.0s\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=30, score=0.6702170017917579, total=  22.0s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60 .\n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60, score=0.6841990871549664, total=  12.0s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60 .\n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60, score=0.6456692913385826, total=  12.1s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60 .\n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100,), splitter__new_len=60, score=0.6829706528249152, total=  11.7s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30, score=0.6582141309059385, total=  14.9s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30, score=0.7278676264373188, total=  13.9s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=30, score=0.7284491339836751, total=  13.4s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60, score=0.6898500326016084, total=   9.9s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60, score=0.7139107611548556, total=  10.2s\n",
      "[CV] mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60 \n",
      "[CV]  mlp__estimator__hidden_layer_sizes=(100, 50), splitter__new_len=60, score=0.7077260930325414, total=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.3min finished\n"
     ]
    }
   ],
   "source": [
    "gs.fit(X, y);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
